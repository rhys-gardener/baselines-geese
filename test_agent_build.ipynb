{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "finnish-trial",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as th\n",
    "from torch import nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import tensor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sapphire-narrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'tmp/best_model.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "wrapped-winning",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(7, 32, kernel_size=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=1)\n",
    "        self.fc3 = nn.Linear(2112, 512)\n",
    "        self.shared1 = nn.Linear(512, 64)\n",
    "        self.policy1 = nn.Linear(64, 32)\n",
    "        self.policy2 = nn.Linear(32, 16)\n",
    "        self.action = nn.Linear(16, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = nn.Flatten()(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.shared1(x))\n",
    "        x = F.relu(self.policy1(x))\n",
    "        x = F.relu(self.policy2(x))\n",
    "        x = self.action(x)\n",
    "        x = x.argmax()\n",
    "        return x\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        #self.shared1 = th.nn.Sequential()\n",
    "        self.policy1 = nn.Linear(231, 64)\n",
    "        self.policy2 = nn.Linear(64, 64)\n",
    "        self.action = nn.Linear(64, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.Flatten(start_dim=1, end_dim=-1)(x)\n",
    "        x = F.tanh(self.policy1(x))\n",
    "        x = F.tanh(self.policy2(x))\n",
    "        x = self.action(x)\n",
    "        x = x.argmax()\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-scottish",
   "metadata": {},
   "source": [
    "mlp_extractor.policy_net.0.weight', 'mlp_extractor.policy_net.0.bias', 'mlp_extractor.policy_net.2.weight', 'mlp_extractor.policy_net.2.bias', 'mlp_extractor.value_net.0.weight', 'mlp_extractor.value_net.0.bias', 'mlp_extractor.value_net.2.weight', 'mlp_extractor.value_net.2.bias', 'action_net.weight', 'action_net.bias', 'value_net.weight', 'value_net.bias'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-delhi",
   "metadata": {},
   "source": [
    "ctorCriticPolicy(  \n",
    "  (features_extractor): FlattenExtractor(  \n",
    "    (flatten): Flatten(start_dim=1, end_dim=-1)  \n",
    "  )  \n",
    "  (mlp_extractor): MlpExtractor(  \n",
    "    (shared_net): Sequential()  \n",
    "    (policy_net): Sequential(  \n",
    "      (0): Linear(in_features=231, out_features=64, bias=True)  \n",
    "      (1): Tanh()  \n",
    "      (2): Linear(in_features=64, out_features=64, bias=True)  \n",
    "      (3): Tanh()  \n",
    "    )  \n",
    "    (value_net): Sequential(  \n",
    "      (0): Linear(in_features=231, out_features=64, bias=True)  \n",
    "      (1): Tanh() \n",
    "      (2): Linear(in_features=64, out_features=64, bias=True)  \n",
    "      (3): Tanh()  \n",
    "    )  \n",
    "  )  \n",
    "  (action_net): Linear(in_features=64, out_features=4, bias=True)  \n",
    "  (value_net): Linear(in_features=64, out_features=1, bias=True)  \n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "announced-maine",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "operating-relay",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = th.load('tmp/best_pytorch_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "unusual-cover",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['actor.latent_pi.0.weight', 'actor.latent_pi.0.bias', 'actor.latent_pi.2.weight', 'actor.latent_pi.2.bias', 'actor.mu.weight', 'actor.mu.bias', 'actor.log_std.weight', 'actor.log_std.bias', 'critic.qf0.0.weight', 'critic.qf0.0.bias', 'critic.qf0.2.weight', 'critic.qf0.2.bias', 'critic.qf0.4.weight', 'critic.qf0.4.bias', 'critic.qf1.0.weight', 'critic.qf1.0.bias', 'critic.qf1.2.weight', 'critic.qf1.2.bias', 'critic.qf1.4.weight', 'critic.qf1.4.bias', 'critic_target.qf0.0.weight', 'critic_target.qf0.0.bias', 'critic_target.qf0.2.weight', 'critic_target.qf0.2.bias', 'critic_target.qf0.4.weight', 'critic_target.qf0.4.bias', 'critic_target.qf1.0.weight', 'critic_target.qf1.0.bias', 'critic_target.qf1.2.weight', 'critic_target.qf1.2.bias', 'critic_target.qf1.4.weight', 'critic_target.qf1.4.bias'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "domestic-burlington",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_geese_observation(state):\n",
    "    \"\"\"\n",
    "    Given a particular geese, does some processing and returns a geese specific observation. \n",
    "    Unfortunately specific to the geese environment for now.\n",
    "    Encoding as follows: \n",
    "    2: enemy snake head\n",
    "    1: enemy snake body\n",
    "    11: own head\n",
    "    12: own body\n",
    "    100: food\n",
    "    \"\"\"\n",
    "\n",
    "    if type(state) is list:\n",
    "        state = state[0].observation\n",
    "    else:\n",
    "\n",
    "        state = Observation(state)\n",
    "\n",
    "    \n",
    "    agent = state.index\n",
    "    rows = 7\n",
    "    columns = 11\n",
    "    game_board_self = np.zeros(rows*columns, None)\n",
    "    game_board_enemy = np.zeros(rows*columns, None)\n",
    "    game_board_food = np.zeros(rows*columns, None)\n",
    "\n",
    "\n",
    "    for i, geese in enumerate(state.geese):\n",
    "        identify=0\n",
    "        if i==agent:\n",
    "            identify=100\n",
    "            for j, cell in enumerate(geese):\n",
    "                if j == 0:\n",
    "                    game_board_self[cell] = identify+1\n",
    "                else:\n",
    "                    game_board_self[cell] = identify+2\n",
    "        else:\n",
    "            identify=-100\n",
    "            for j, cell in enumerate(geese):\n",
    "                if j == 0:\n",
    "                    game_board_enemy[cell] = identify+1\n",
    "                else:\n",
    "                    game_board_enemy[cell] = identify+2\n",
    "\n",
    "    for food in state.food:\n",
    "        game_board_food[food] = 1000\n",
    "    game_board_self = game_board_self.reshape([rows, columns])\n",
    "    game_board_enemy = game_board_enemy.reshape([rows, columns])\n",
    "    game_board_food = game_board_food.reshape([rows, columns])\n",
    "\n",
    "    head = get_geese_coord(game_board_self)[0]\n",
    "\n",
    "\n",
    "    game_board_self = np.roll(game_board_self, 5-head[1], axis=1)\n",
    "    game_board_self = np.roll(game_board_self, 3-head[0], axis=0)\n",
    "    game_board_enemy = np.roll(game_board_enemy, 5-head[1], axis=1)\n",
    "    game_board_enemy = np.roll(game_board_enemy, 3-head[0], axis=0)\n",
    "    game_board_food = np.roll(game_board_food, 5-head[1], axis=1)\n",
    "    game_board_food = np.roll(game_board_food, 3-head[0], axis=0)\n",
    "\n",
    "    #game_board = game_board.reshape((game_board.shape[0], game_board.shape[1], 1))\n",
    "    game_board = np.dstack((game_board_self, game_board_enemy, game_board_food))\n",
    "    return game_board\n",
    "\n",
    "def get_geese_coord(board):\n",
    "    return get_coord_from_np_grid(board, 101)\n",
    "\n",
    "def get_food_coord(board):\n",
    "    return get_coord_from_np_grid(board, 1000)\n",
    "\n",
    "def get_enemy_geese_head_coord(board):\n",
    "    return get_coord_from_np_grid(board, -99)\n",
    "\n",
    "\n",
    "def get_coord_from_np_grid(grid, value):\n",
    "    coords = []\n",
    "    for i in range(0, len(np.where(grid==value)[0])):\n",
    "        coords.append((np.where(grid==value)[0][i], np.where(grid==value)[1][i]))\n",
    "    return coords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "supported-investigator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nstate_dict = {\\n    'conv1.weight': state_dict['features_extractor.conv1.weight'],\\n    'conv1.bias': state_dict['features_extractor.conv1.bias'],\\n    'conv2.weight': state_dict['features_extractor.conv2.weight'],\\n    'conv2.bias': state_dict['features_extractor.conv2.bias'],\\n    'fc3.weight': state_dict['features_extractor.fc3.weight'],\\n    'fc3.bias': state_dict['features_extractor.fc3.bias'],\\n\\n    'shared1.weight': state_dict['mlp_extractor.shared_net.0.weight'],\\n    'shared1.bias': state_dict['mlp_extractor.shared_net.0.bias'],\\n\\n    'policy1.weight': state_dict['mlp_extractor.policy_net.0.weight'],\\n    'policy1.bias': state_dict['mlp_extractor.policy_net.0.bias'],\\n    'policy2.weight': state_dict['mlp_extractor.policy_net.2.weight'],\\n    'policy2.bias': state_dict['mlp_extractor.policy_net.2.bias'],\\n\\n    'action.weight': state_dict['action_net.weight'],\\n    'action.bias': state_dict['action_net.bias'],\\n}\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "state_dict = {\n",
    "    'conv1.weight': state_dict['features_extractor.conv1.weight'],\n",
    "    'conv1.bias': state_dict['features_extractor.conv1.bias'],\n",
    "    'conv2.weight': state_dict['features_extractor.conv2.weight'],\n",
    "    'conv2.bias': state_dict['features_extractor.conv2.bias'],\n",
    "    'fc3.weight': state_dict['features_extractor.fc3.weight'],\n",
    "    'fc3.bias': state_dict['features_extractor.fc3.bias'],\n",
    "\n",
    "    'shared1.weight': state_dict['mlp_extractor.shared_net.0.weight'],\n",
    "    'shared1.bias': state_dict['mlp_extractor.shared_net.0.bias'],\n",
    "\n",
    "    'policy1.weight': state_dict['mlp_extractor.policy_net.0.weight'],\n",
    "    'policy1.bias': state_dict['mlp_extractor.policy_net.0.bias'],\n",
    "    'policy2.weight': state_dict['mlp_extractor.policy_net.2.weight'],\n",
    "    'policy2.bias': state_dict['mlp_extractor.policy_net.2.bias'],\n",
    "\n",
    "    'action.weight': state_dict['action_net.weight'],\n",
    "    'action.bias': state_dict['action_net.bias'],\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "polish-player",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(obs, config):\n",
    "    model = ConvNet()\n",
    "    model = model.float()\n",
    "    state_dict = th.load('tmp/best_pytorch_model')\n",
    "    state_dict = {\n",
    "        'conv1.weight': state_dict['features_extractor.conv1.weight'],\n",
    "        'conv1.bias': state_dict['features_extractor.conv1.bias'],\n",
    "        'conv2.weight': state_dict['features_extractor.conv2.weight'],\n",
    "        'conv2.bias': state_dict['features_extractor.conv2.bias'],\n",
    "        'fc3.weight': state_dict['features_extractor.fc3.weight'],\n",
    "        'fc3.bias': state_dict['features_extractor.fc3.bias'],\n",
    "\n",
    "        'shared1.weight': state_dict['mlp_extractor.shared_net.0.weight'],\n",
    "        'shared1.bias': state_dict['mlp_extractor.shared_net.0.bias'],\n",
    "\n",
    "        'policy1.weight': state_dict['mlp_extractor.policy_net.0.weight'],\n",
    "        'policy1.bias': state_dict['mlp_extractor.policy_net.0.bias'],\n",
    "        'policy2.weight': state_dict['mlp_extractor.policy_net.2.weight'],\n",
    "        'policy2.bias': state_dict['mlp_extractor.policy_net.2.bias'],\n",
    "\n",
    "        'action.weight': state_dict['action_net.weight'],\n",
    "        'action.bias': state_dict['action_net.bias'],\n",
    "    }\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    obs = tensor(get_geese_observation(obs)).reshape(1, 7, 11, 3).float()\n",
    "    action = model(obs)\n",
    "    print(action)\n",
    "    return int(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "becoming-palmer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading environment football failed: No module named 'gfootball'\n"
     ]
    }
   ],
   "source": [
    "from GeeseGymWrapper import HungryGeeseKaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "prompt-referral",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = HungryGeeseKaggle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "increased-spectrum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'action': 'NORTH',\n",
       "  'reward': 0,\n",
       "  'info': {},\n",
       "  'observation': {'remainingOverageTime': 60,\n",
       "   'step': 0,\n",
       "   'geese': [[10], [72], [4], [70]],\n",
       "   'food': [29, 60],\n",
       "   'index': 0},\n",
       "  'status': 'ACTIVE'},\n",
       " {'action': 'NORTH',\n",
       "  'reward': 0,\n",
       "  'info': {},\n",
       "  'observation': {'remainingOverageTime': 60, 'index': 1},\n",
       "  'status': 'ACTIVE'},\n",
       " {'action': 'NORTH',\n",
       "  'reward': 0,\n",
       "  'info': {},\n",
       "  'observation': {'remainingOverageTime': 60, 'index': 2},\n",
       "  'status': 'ACTIVE'},\n",
       " {'action': 'NORTH',\n",
       "  'reward': 0,\n",
       "  'info': {},\n",
       "  'observation': {'remainingOverageTime': 60, 'index': 3},\n",
       "  'status': 'ACTIVE'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.env.reset(num_agents=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "floppy-utility",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.env.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "collect-potential",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'action': 'NORTH',\n",
       "  'reward': 0,\n",
       "  'info': {},\n",
       "  'observation': {'remainingOverageTime': 60,\n",
       "   'step': 0,\n",
       "   'geese': [[10], [72], [4], [70]],\n",
       "   'food': [29, 60],\n",
       "   'index': 0},\n",
       "  'status': 'ACTIVE'},\n",
       " {'action': 'NORTH',\n",
       "  'reward': 0,\n",
       "  'info': {},\n",
       "  'observation': {'remainingOverageTime': 60, 'index': 1},\n",
       "  'status': 'ACTIVE'},\n",
       " {'action': 'NORTH',\n",
       "  'reward': 0,\n",
       "  'info': {},\n",
       "  'observation': {'remainingOverageTime': 60, 'index': 2},\n",
       "  'status': 'ACTIVE'},\n",
       " {'action': 'NORTH',\n",
       "  'reward': 0,\n",
       "  'info': {},\n",
       "  'observation': {'remainingOverageTime': 60, 'index': 3},\n",
       "  'status': 'ACTIVE'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "obvious-particular",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eligible-strategy",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ConvNet:\n\tsize mismatch for conv1.weight: copying a param with shape torch.Size([32, 3, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 7, 1, 1]).\n\tsize mismatch for fc3.weight: copying a param with shape torch.Size([512, 4928]) from checkpoint, the shape in current model is torch.Size([512, 2112]).\n\tsize mismatch for shared1.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 512]).\n\tsize mismatch for shared1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for policy1.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([32, 64]).\n\tsize mismatch for policy1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for policy2.weight: copying a param with shape torch.Size([32, 256]) from checkpoint, the shape in current model is torch.Size([16, 32]).\n\tsize mismatch for policy2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for action.weight: copying a param with shape torch.Size([4, 32]) from checkpoint, the shape in current model is torch.Size([4, 16]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-882b641c6b6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-a21172beded8>\u001b[0m in \u001b[0;36magent\u001b[0;34m(obs, config)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;34m'action.bias'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action_net.bias'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     }\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_geese_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/baselines-geese/baselines-geese/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   1224\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   1225\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ConvNet:\n\tsize mismatch for conv1.weight: copying a param with shape torch.Size([32, 3, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 7, 1, 1]).\n\tsize mismatch for fc3.weight: copying a param with shape torch.Size([512, 4928]) from checkpoint, the shape in current model is torch.Size([512, 2112]).\n\tsize mismatch for shared1.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([64, 512]).\n\tsize mismatch for shared1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for policy1.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([32, 64]).\n\tsize mismatch for policy1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for policy2.weight: copying a param with shape torch.Size([32, 256]) from checkpoint, the shape in current model is torch.Size([16, 32]).\n\tsize mismatch for policy2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for action.weight: copying a param with shape torch.Size([4, 32]) from checkpoint, the shape in current model is torch.Size([4, 16])."
     ]
    }
   ],
   "source": [
    "agent(obs, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desirable-chapel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-republican",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "republican-tutorial",
   "metadata": {},
   "outputs": [],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-commons",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that if I load the same model from stable baselines I get the same output\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "log_dir = \"tmp/\"\n",
    "env = HungryGeeseKaggle()\n",
    "env = Monitor(env, log_dir)\n",
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "available-future",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.load('best_model.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-tamil",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conventional-timber",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy.predict(get_geese_observation(obs), deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-madrid",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-petersburg",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baselines-geese",
   "language": "python",
   "name": "baselines-geese"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
